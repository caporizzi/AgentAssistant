{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.11/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "Running on public URL: https://6f820ede1f6106afb0.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6f820ede1f6106afb0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the Ollama LLaMA model\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=60)\n",
    "\n",
    "# Initialize OpenAI client for TTS\n",
    "client = OpenAI()\n",
    "\n",
    "def chat(input_text):\n",
    "    # Generate response using LLaMA model\n",
    "    response = llm.complete(input_text)\n",
    "    response_text = response.text\n",
    "    \n",
    "    # Convert the response text to speech using OpenAI's TTS API\n",
    "    audio_file_path = generate_speech(response_text)\n",
    "    \n",
    "    # Return the response text and the audio file path for Gradio\n",
    "    return response_text, audio_file_path\n",
    "\n",
    "def generate_speech(text):\n",
    "    # Generate speech using OpenAI's TTS API without streaming response\n",
    "    speech = client.audio.speech.create(\n",
    "        model=\"tts-1\",  # You can choose \"tts-1\" or \"tts-1-hd\"\n",
    "        voice=\"alloy\",  # Choose the voice you prefer (\"alloy\", \"echo\", etc.)\n",
    "        input=text,\n",
    "        response_format=\"mp3\"  # Specify the response format (e.g., mp3)\n",
    "    )\n",
    "\n",
    "    # Get the binary response content (audio data)\n",
    "    audio_data = speech.content  # Access the 'content' attribute\n",
    "\n",
    "    # Define the audio file path based on the current working directory\n",
    "    audio_file_path = os.path.join(os.getcwd(), \"response_audio.mp3\")\n",
    "    \n",
    "    # Write the binary content to the audio file\n",
    "    with open(audio_file_path, \"wb\") as f:\n",
    "        f.write(audio_data)  # Write the raw binary data\n",
    "\n",
    "    # Return the path to the saved audio file\n",
    "    return audio_file_path\n",
    "\n",
    "# Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=chat, \n",
    "    inputs=\"text\", \n",
    "    outputs=[\"text\", \"audio\"],  # Output text and audio file\n",
    "    title=\"LLaMA 3 Chatbot with OpenAI TTS\",\n",
    "    description=\"Chat with a LLaMA 3-based model and hear the response via OpenAI's TTS!\"\n",
    ")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
