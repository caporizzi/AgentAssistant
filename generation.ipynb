{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pysqlite3\n",
    "import sys\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n",
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import VectorStore\n",
    "from typing import List, Any\n",
    "from chromadb import Client\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch `create_collection` to `get_or_create_collection` to avoid creating a new collection every time\n",
    "collection_name=\"dwarf_collectors\"\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom LangChain wrapper for the Ollama LLaMA model\n",
    "class OllamaLangChain(LLM):\n",
    "    model_name: str = \"llama3.2:latest\"  # Declared field\n",
    "    request_timeout: int = 90           # Declared field\n",
    "\n",
    "    def __init__(self, model_name=\"llama3.2:latest\", request_timeout=30):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.request_timeout = request_timeout\n",
    "        # Use object.__setattr__ to bypass Pydantic's field validation\n",
    "        object.__setattr__(self, \"_client\", Ollama(model=model_name, request_timeout=request_timeout))\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"ollama\"\n",
    "\n",
    "    def _call(self, prompt, stop=None):\n",
    "        # Access the private client\n",
    "        client = object.__getattribute__(self, \"_client\")\n",
    "        response = client.complete(prompt)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open(\"docs/dwarf.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Create a list of a single document\n",
    "documents = [Document(page_content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 256,\n",
    "    chunk_overlap  = 40\n",
    "    \n",
    ")\n",
    "# Split the text into chunks\n",
    "splitted_txt = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwarf\n",
      "Dwarf Traits\n",
      "Your dwarf character has an assortment of inborn\n",
      "abilities, part and parcel of dwarven nature.\n",
      "Ability Score Increase. Your Constitution score\n",
      "increases by 2.\n",
      "Age. Dwarves mature at the same rate as humans,\n",
      "\n",
      "\n",
      "but they’re considered young until they reach the\n",
      "age of 50. On average, they live about 350 years.\n",
      "Alignment. Most dwarves are lawful, believing\n",
      "firmly in the benefits of a well-­‐‑ordered society. They\n",
      "\n",
      "\n",
      "tend toward good as well, with a strong sense of fair\n",
      "play and a belief that everyone deserves to share in\n",
      "the benefits of a just order.\n",
      "Size. Dwarves stand between 4 and 5 feet tall and\n",
      "average about 150 pounds. Your size is Medium.\n",
      "\n",
      "\n",
      "Speed. Your base walking speed is 25 feet. Your\n",
      "speed is not reduced by wearing heavy armor.\n",
      "Darkvision. Accustomed to life underground, you\n",
      "have superior vision in dark and dim conditions. You\n",
      "can see in dim light within 60 feet of you as if it were\n",
      "\n",
      "\n",
      "bright light, and in darkness as if it were dim light.\n",
      "You can’t discern color in darkness, only shades of\n",
      "gray.\n",
      "Dwarven Resilience. You have advantage on\n",
      "saving throws against poison, and you have\n",
      "resistance against poison damage.\n",
      "\n",
      "\n",
      "resistance against poison damage.\n",
      "Dwarven Combat Training. You have proficiency\n",
      "with the battleaxe, handaxe, light hammer, and\n",
      "warhammer.\n",
      "Tool Proficiency. You gain proficiency with the\n",
      "artisan’s tools of your choice: smith’s tools, brewer’s\n",
      "\n",
      "\n",
      "supplies, or mason’s tools.\n",
      "Stonecunning. Whenever you make an\n",
      "Intelligence (History) check related to the origin of\n",
      "stonework, you are considered proficient in the\n",
      "History skill and add double your proficiency bonus\n",
      "\n",
      "\n",
      "to the check, instead of your normal proficiency\n",
      "bonus.\n",
      "Languages. You can speak, read, and write\n",
      "Common and Dwarvish. Dwarvish is full of hard\n",
      "consonants and guttural sounds, and those\n",
      "characteristics spill over into whatever other\n",
      "\n",
      "\n",
      "language a dwarf might speak.\n",
      "System Reference Document 5.1 3\n",
      "\n",
      "\n",
      "Hill Dwarf\n",
      "As a hill dwarf, you have keen senses, deep intuition,\n",
      "and remarkable resilience.\n",
      "Ability Score Increase. Your Wisdom score\n",
      "increases by 1.\n",
      "Dwarven Toughness. Your hit point maximum\n",
      "increases by 1, and it increases by 1 every time you\n",
      "\n",
      "\n",
      "gain a level.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output the result\n",
    "for chunk in splitted_txt:\n",
    "    print(chunk.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    client=chroma_client\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['57ea77e7-bdc0-4c95-9892-63f952d6d44a',\n",
       " 'a4002c0d-d669-4b4d-ab85-a7aa3ee42988',\n",
       " '59e95f09-68bb-4f88-9814-9daf14556863',\n",
       " '77d336c4-2b7a-4ae0-a935-292b17bb08fe',\n",
       " '4abd1d85-b29d-4ee2-95ac-b94dd1340fdd',\n",
       " '7eebb113-0fd5-4105-969c-b0c237fbabd6',\n",
       " '05da72dd-1a0e-42cb-8e0e-0ba5b6fc10ee',\n",
       " '22ccdbdc-0ed6-414f-9c11-20b270f93590',\n",
       " 'c017197a-3fa9-4808-8b8e-a545c75076ad',\n",
       " 'f878b2f6-5e55-4a23-8b30-d9f576e1d718',\n",
       " '9d39c731-0f30-4483-9071-16391b08daa0']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add documents to the collection\n",
    "vectorstore.add_documents(splitted_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLangChain()\n",
    "memory = ConversationBufferMemory()  # Store chat history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As a Dwarf, you can speak, read, and write Common and Dwarvish. However, speaking another language may not be as proficient due to the unique characteristics of Dwarvish, which often influence other languages spoken by Dwarves. This can result in a lower proficiency bonus when trying to communicate in a foreign tongue.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define your custom prompt template\n",
    "custom_prompt_template = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Keep the answer concise, with a maximum of three sentences.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate using your custom template\n",
    "custom_prompt = PromptTemplate(template=custom_prompt_template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def get_context_for_question(question):\n",
    "    # Create a VectorStoreRetriever using 'as_retriever()' on your vectorstore\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})  # Adjust as needed\n",
    "    \n",
    "    # Retrieve the relevant documents based on the input question\n",
    "    context = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Format the retrieved context into a string\n",
    "    formatted_context = format_docs(context)\n",
    "    return formatted_context\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        #\"context\": RunnablePassthrough(func=get_context_for_question),\n",
    "        \"context\": vectorstore.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    \n",
    ")\n",
    "\n",
    "qa_chain.invoke(\"What about language in Dwarf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to handle the Gradio interface input/output\n",
    "def answer_question(question):\n",
    "    # Use the qa_chain to answer the question\n",
    "    return qa_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "Running on public URL: https://9ec9bc2aefb0b2fc0c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9ec9bc2aefb0b2fc0c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"LLaMA 3 Chatbot with LangChain\",\n",
    "    description=\"Chat with a LLaMA 3-based model via Ollama and LangChain!\"\n",
    ")\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
