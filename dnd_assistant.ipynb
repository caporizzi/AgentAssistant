{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pysqlite3\n",
    "import sys\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:732: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Resume les traits des Tieffelin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(query):\n",
    "    # Detect the language of the query\n",
    "    language = detect(query)\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n"
     ]
    }
   ],
   "source": [
    "lang = route_query(query)\n",
    "print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sources\n",
    "read_pdf = True\n",
    "read_web_online = False\n",
    "read_csv = False\n",
    "\n",
    "de = lang == \"de\"\n",
    "en = lang == \"en\"\n",
    "fr = lang == \"fr\"\n",
    "# All data will be in the list documents\n",
    "documents=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "if read_pdf:\n",
    "    if en:\n",
    "        # Load and process the text files\n",
    "        # loader = TextLoader('single_text_file.txt')\n",
    "        loader = DirectoryLoader('./docs_en/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "        \n",
    "        pdf_docs = loader.load()\n",
    "        len(pdf_docs)\n",
    "\n",
    "        # tokenize pdf\n",
    "        documents.extend(text_splitter.split_documents(pdf_docs))\n",
    "        len(documents)\n",
    "    if de:\n",
    "        # Load and process the text files\n",
    "        # loader = TextLoader('single_text_file.txt')\n",
    "        loader = DirectoryLoader('./docs_de/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "        \n",
    "        pdf_docs = loader.load()\n",
    "        len(pdf_docs)\n",
    "\n",
    "        # tokenize pdf\n",
    "        documents.extend(text_splitter.split_documents(pdf_docs))\n",
    "        len(documents)\n",
    "    if fr:\n",
    "        # Load and process the text files\n",
    "        # loader = TextLoader('single_text_file.txt')\n",
    "        loader = DirectoryLoader('./docs_fr/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "        \n",
    "        pdf_docs = loader.load()\n",
    "        len(pdf_docs)\n",
    "\n",
    "        # tokenize pdf\n",
    "        documents.extend(text_splitter.split_documents(pdf_docs))\n",
    "        len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader  # Read one or a list of pages\n",
    "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader  # Read recursively from a root page\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "# Initialize variables\n",
    "is_recursively = False\n",
    "documents = []  # Ensure this is defined\n",
    "\n",
    "# Conditional loading\n",
    "if is_recursively:\n",
    "    url = \"https://www.aidedd.org/regles/races\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url,\n",
    "        max_depth=5,  # Consider a smaller depth for efficiency\n",
    "        extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    web_docs_up = loader.load()\n",
    "else:\n",
    "    urls = [\n",
    "        \"https://www.aidedd.org/regles/races\",\n",
    "    ]\n",
    "    loader = WebBaseLoader(urls)\n",
    "    web_docs_up = loader.load()\n",
    "\n",
    "# Tokenize web online\n",
    "# Ensure text_splitter is defined before this step\n",
    "try:\n",
    "    web_docs = text_splitter.split_documents(web_docs_up)\n",
    "\n",
    "    # Define or import filter_complex_metadata\n",
    "    def filter_complex_metadata(docs):\n",
    "        # Placeholder for actual filtering logic\n",
    "        return docs\n",
    "\n",
    "    # Filter documents and extend the list\n",
    "    documents.extend(filter_complex_metadata(web_docs))\n",
    "    print(f\"Number of documents: {len(documents)}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_csv:\n",
    "    # Load the CSV file\n",
    "    loader = CSVLoader(file_path=\"docs/*.csv\")\n",
    "    data = loader.load_and_split()\n",
    "    \n",
    "    # we filter out metadata that the embedding below will not be able to manage\n",
    "    data = filter_complex_metadata(data)\n",
    "    documents.extend(data)\n",
    "    \n",
    "    len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 - Page Number: N/A\n",
      "Content Preview:\n",
      "Interdit à la revente. Vous êtes autorisés à imprimer ou photocopier  \n",
      "ce document pour votre strict usage personnel .  Document de Référence du Système 5.1  2 Document de Référence du  Système  5.1 \n",
      " \n",
      "Si vous constatez des erreurs dans ce document, veuillez \n",
      "nous en informer par e -mail à l’adresse askdnd@wizards.com . \n",
      "Races  \n",
      "Traits raciaux  \n",
      "La description de chaque race comprend des traits \n",
      "raciaux communs aux membres de ce peuple. Les entrées ci -après figurent parmi les traits de \n",
      "la plupart des races.  \n",
      "Âge \n",
      "L’entrée « Âge » indique l’âge auquel un membre \n",
      "de la race est considéré comme adulte, ainsi que \n",
      "l’espérance de vie moyenne propre à ce peuple. Cette information est prévue pour vous aider à déterminer l’âge initial de votre personnage en début d’aventure.  \n",
      "Vous êtes cependant libre de décider de l’âge de votre \n",
      "personnage, ce qui peut expliquer telle ou telle valeur  \n",
      "de caractéristique. Si, par exemple, vous incarnez un \n",
      "personnage jeune ou très âgé, cet âge peut expliquer \n",
      "une valeur de Force ou de Constitution particulièrement  \n",
      "faible, tandis qu’un âge avancé peut justifier une \n",
      "Intelligence ou une Sagesse élevée.  \n",
      "Alignement  \n",
      "La plupart des races s’orientent vers l’un ou l’autre alignement, comme exposé dans cette entrée. Les personnages joueurs ne sont pas tenus  de respecter \n",
      "cette tendance, mais réfléchir aux raisons qui font \n",
      "que votre nain est chaotique, par exemple, à rebours \n",
      "d’une société naine très majoritairement loyale, permet de mieux définir votre personnage.  \n",
      "Augmentation de caractéristique  \n",
      "Chaque race augmente une ou plusieurs valeurs de caractéristique du personnage.  \n",
      "Catégorie de taille  \n",
      "Les personnages de la plupart des races sont de taille M, une catégorie de taille qui comprend les \n",
      "créatures mesurant entre 1,20  m et 2,40  m environ. \n",
      "Les membres de quelques races sont de taille P \n",
      "(entre 60  cm et 1,20  m de haut), ce qui signifie que \n",
      "certaines règles du jeu les affectent différemment.  La plus importante de ces règles est que les \n",
      "personnages de taille P sont mal à l’aise dans le maniement des armes lourdes , comme expliqué \n",
      "à la section «  Équipement ». \n",
      "Vitesse  \n",
      "La vitesse d’un personnage détermine la distance \n",
      "qu’il peut parcourir en voyageant («  Aventure  ») \n",
      "et en combattant («  Combat  »). \n",
      "Langues  \n",
      "Selon sa race, un personnage parle, lit et écrit \n",
      "certaines langues.  \n",
      "Variantes raciales  \n",
      "Certaines races proposent des variantes raciales. Les membres d’une telle variante raciale ont accès \n",
      "aux traits de la race parente en plus des traits spécifiés  \n",
      "dans la variante raciale. Les relations entre les diverses  \n",
      "variantes raciales sont très variables d’un peuple et d’un monde à l’autre.  \n",
      " \n",
      "Elfe \n",
      "Traits des elfes  \n",
      "Votre personnage elfe possède un ensemble d’aptitudes naturelles, fruit de millénaires de raffinement elfique.  \n",
      " Augmentation de caractéristique. Votre valeur \n",
      "de Dextérité augmente de 2.  \n",
      " Âge.  Bien que les elfes atteignent leur maturité \n",
      "corporelle au même âge que les humains, ils  \n",
      "considèrent  qu’un adulte n’est pas juste physiquement  \n",
      "développé, mais aussi un être d’expérience. Les elfes se déclarent généralement adultes et adoptent leur nom définitif aux alentours de leurs 100  ans. Ils peuvent \n",
      "vivre jusqu’à 750  ans....\n",
      "\n",
      "Document 2 - Page Number: N/A\n",
      "Content Preview:\n",
      "vivre jusqu’à 750  ans.  \n",
      " Alignement.  Les elfes prisent la liberté, la variété \n",
      "et l’expression de chacun, si bien qu’ils sont enc lins \n",
      "au chaos sous ses formes les plus douces. La liberté \n",
      "d’autrui mérite autant d’être protégée que la leur, c’est pourquoi le bien a tendance à obtenir leur adhésion.  \n",
      " Catégorie de taille.  Les elfes mesurent entre \n",
      "1,50  m et 1,80  m, avec une carrure plutô t fine. \n",
      "Vous  êtes de taille M. \n",
      " Vitesse.  Votre vitesse de base au sol est de 9  m. \n",
      " Vision ans le noir.  Habitué aux forêts vespérales \n",
      "et au ciel nocturne, vous disposez d’une vision...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the documents\n",
    "for idx, doc in enumerate(documents, start=1):  # Enumerate to track the document index\n",
    "    # Safely access metadata and page content\n",
    "    page_number = doc.metadata.get(\"page_number\", \"N/A\")  # Default to 'N/A' if 'page_number' is not present\n",
    "    content_preview = doc.page_content[:10000] if doc.page_content else \"No content\"  # Handle missing content gracefully\n",
    "\n",
    "    # Print the page number and a preview of the content\n",
    "    print(f\"Document {idx} - Page Number: {page_number}\\nContent Preview:\\n{content_preview}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom LangChain wrapper for the Ollama LLaMA model\n",
    "class OllamaLangChain(LLM):\n",
    "    model_name: str = \"llama3.2:latest\"  # Declared field\n",
    "    request_timeout: int = 9000          # Declared field\n",
    "\n",
    "    def __init__(self, model_name=\"llama3.2:latest\", request_timeout=9000):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.request_timeout = request_timeout\n",
    "        # Use object.__setattr__ to bypass Pydantic's field validation\n",
    "        object.__setattr__(self, \"_client\", Ollama(model=model_name, request_timeout=request_timeout))\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"ollama\"\n",
    "\n",
    "    def _call(self, prompt, stop=None):\n",
    "        # Access the private client\n",
    "        client = object.__getattribute__(self, \"_client\")\n",
    "        response = client.complete(prompt)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lang == \"fr\" or lang == \"de\":\n",
    "    embeddings = OllamaEmbeddings(\n",
    "        model=\"bge-m3\"\n",
    "    )\n",
    "else:\n",
    "    embeddings = OllamaEmbeddings(\n",
    "        model=\"llama3.2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Set up a persist directory\n",
    "persist_directory = \"./chroma_persistent_data\"\n",
    "\n",
    "# Use get_or_create_collection to avoid re-creating collection each time\n",
    "collection_name = f\"{lang}_cn1\"\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "# Create the vector store using the existing collection\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    client=chroma_client,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_vectorstore(vectorstore, documents):\n",
    "    # Extract text and metadata from the documents\n",
    "    text = [document.page_content for document in documents]\n",
    "    metadata = [document.metadata for document in documents if document.metadata]\n",
    "\n",
    "    # Ensure embedding function is set\n",
    "    if vectorstore._embedding_function is None:\n",
    "        raise ValueError(\"Embedding function must be set.\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = []\n",
    "    for idx, doc_text in enumerate(text):\n",
    "        print(f\"Generating embedding for document {idx + 1}/{len(text)}...\")\n",
    "        embedding = vectorstore._embedding_function.embed_documents([doc_text])[0]\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Add documents to the vector store\n",
    "    ids = [str(i) for i in range(len(text))]\n",
    "    vectorstore._collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=embeddings,\n",
    "        documents=text,\n",
    "        metadatas=metadata,\n",
    "    )\n",
    "    print(f\"Successfully added {len(documents)} documents to the vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for document 1/2...\n",
      "Generating embedding for document 2/2...\n",
      "Successfully added 2 documents to the vector store.\n",
      "Successfully added 2 documents to the vector store.\n"
     ]
    }
   ],
   "source": [
    "add_documents_to_vectorstore(vectorstore, documents)\n",
    "\n",
    "print(f\"Successfully added {len(documents)} documents to the vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLangChain()\n",
    "memory = ConversationBufferMemory()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prompt: input_variables=['context', 'question'] metadata={'lc_hub_owner': 'kpikpo', 'lc_hub_repo': 'dnd_prompt_fr', 'lc_hub_commit_hash': 'aec2effc0779b3b52eb624247d175e308b29dde841a20272f6d75f882c7b4ce4'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Analysez le contexte en profondeur pour répondre à la question. Si la réponse n'est pas trouvée, reconnaissez-le. En tant qu'expert de Donjons et Dragons, fournissez des informations détaillées lorsque c'est possible, en utilisant un maximum de 10 phrases.\\nQuestion : {question}\\nContext : {context}\\nRéponse :\"))]\n"
     ]
    }
   ],
   "source": [
    "prompt_mapping = {\n",
    "    \"en\": \"kpikpo/dnd_prompt\",\n",
    "    \"de\": \"kpikpo/ger\",\n",
    "    \"fr\": \"kpikpo/dnd_prompt_fr\"\n",
    "}\n",
    "\n",
    "# Use the detected language to pull the appropriate prompt\n",
    "prompt = hub.pull(prompt_mapping.get(lang, \"default/prompt\"))\n",
    "\n",
    "print(f\"Using prompt: {prompt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using reflect prompt for language 'fr':\n",
      "Debute en disant Hmmm\n",
      "\n",
      "Vous avez fourni la réponse suivante en fonction du contexte donné :\n",
      "\n",
      "Réponse :\n",
      "{response}\n",
      "\n",
      "Contexte :\n",
      "{context}\n",
      "\n",
      "Réfléchissez de manière critique à la réponse. Répond-elle pleinement à la question en fonction du contexte fourni ? Identifiez les éventuelles inexactitudes, lacunes ou domaines à améliorer, et révisez la réponse en conséquence pour garantir qu'elle soit concise, précise et contextuellement pertinente. Si la réponse est satisfaisante, confirmez-le en l'état.\n"
     ]
    }
   ],
   "source": [
    "# Mapping languages to their respective reflection prompts\n",
    "reflect_prompt_mapping = {\n",
    "    \"en\": (\n",
    "        \"Always start saying Hmmm\\n\\n\"\n",
    "        \"You provided the following response based on the given context:\\n\\n\"\n",
    "        \"Response:\\n{response}\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Reflect critically on the response. Does it fully answer the question based on the provided context? \"\n",
    "        \"Identify any inaccuracies, gaps, or areas for improvement, and revise the response accordingly to ensure it is concise, accurate, and contextually relevant. \"\n",
    "        \"If the response is satisfactory, confirm it as is.\"\n",
    "    ),\n",
    "    \"de\": (\n",
    "        \"Anfag mit Hmmmm\\n\\n\"\n",
    "        \"Sie haben die folgende Antwort basierend auf dem gegebenen Kontext bereitgestellt:\\n\\n\"\n",
    "        \"Antwort:\\n{response}\\n\\n\"\n",
    "        \"Kontext:\\n{context}\\n\\n\"\n",
    "        \"Reflektieren Sie kritisch über die Antwort. Beantwortet sie die Frage vollständig basierend auf dem bereitgestellten Kontext? \"\n",
    "        \"Identifizieren Sie eventuelle Ungenauigkeiten, Lücken oder Verbesserungsbereiche und überarbeiten Sie die Antwort entsprechend, \"\n",
    "        \"um sicherzustellen, dass sie präzise, korrekt und kontextbezogen ist. \"\n",
    "        \"Falls die Antwort zufriedenstellend ist, bestätigen Sie dies entsprechend.\"\n",
    "    ),\n",
    "    \"fr\": (\n",
    "        \"Debute en disant Hmmm\\n\\n\"\n",
    "        \"Vous avez fourni la réponse suivante en fonction du contexte donné :\\n\\n\"\n",
    "        \"Réponse :\\n{response}\\n\\n\"\n",
    "        \"Contexte :\\n{context}\\n\\n\"\n",
    "        \"Réfléchissez de manière critique à la réponse. Répond-elle pleinement à la question en fonction du contexte fourni ? \"\n",
    "        \"Identifiez les éventuelles inexactitudes, lacunes ou domaines à améliorer, et révisez la réponse en conséquence pour garantir qu'elle soit concise, précise et contextuellement pertinente. \"\n",
    "        \"Si la réponse est satisfaisante, confirmez-le en l'état.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Use the detected language to select the appropriate reflection prompt\n",
    "reflect_prompt = reflect_prompt_mapping.get(lang, \"Default reflection prompt if language is unsupported\")\n",
    "\n",
    "print(f\"Using reflect prompt for language '{lang}':\\n{reflect_prompt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_documents):\n",
    "    # Format documents to include content and metadata\n",
    "    formatted_docs = []\n",
    "    for doc in retrieved_documents:\n",
    "        content = doc.page_content\n",
    "        metadata = doc.metadata  # Extract metadata\n",
    "\n",
    "        formatted_docs.append({\n",
    "            \"content\": content,\n",
    "            \"metadata\": metadata  # Add metadata to the formatted output\n",
    "        })\n",
    "\n",
    "    return formatted_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "retrieved_documents = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_reflection = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self_reflection:\n",
    "    \n",
    "    qa_chain = (\n",
    "        {\n",
    "            \"context\": vectorstore.as_retriever() | format_docs,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        | (  # Self-reflection step\n",
    "            lambda response: reflect_prompt.format(\n",
    "                response=response,\n",
    "                context=format_docs(retrieved_documents)\n",
    "            )  # Returns a string directly\n",
    "        )\n",
    "        | llm  # Directly pass the reflection prompt as input to llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "else:\n",
    "    qa_chain = (\n",
    "        {\n",
    "            \"context\": vectorstore.as_retriever() | format_docs,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Bonjour ! Je vais analyser votre réponse et identifier les points forts et les points faibles.\\n\\nPoints forts :\\n\\n* Vous avez identifié le contexte de la question, qui concerne les variantes raciales dans Donjons et Dragons.\\n* Vous avez mentionné que vous ne pouvez pas fournir une réponse spécifique à cette question en raison du manque de contenus fournis.\\n* Vous avez proposé d'obtenir plus d'informations sur les variantes raciales en consultant les manuels officiels ou les ressources en ligne dédiées à Donjons et Dragons.\\n\\nPoints faibles :\\n\\n* Votre réponse est très générale et ne répond pas pleinement à la question. Vous avez mentionné que vous ne pouvez pas fournir une réponse spécifique, mais vous n'avez pas expliqué pourquoi.\\n* Vous n'avez pas analysé les informations contenues dans le document fourni pour identifier les variantes raciales des elfes.\\n* Votre réponse manque de concision et de précision. Vous avez mentionné plusieurs points sans fournir d'explications détaillées.\\n\\nRévision :\\n\\nPour répondre pleinement à la question, je vais réviser ma réponse en conséquence. Voici une nouvelle réponse qui est concise, précise et contextuellement pertinente :\\n\\nLa question demande des informations sur les variantes raciales des elfes dans Donjons et Dragons. Selon le document fourni, il n'y a pas de variante raciale spécifique mentionnée pour les elfes. Cependant, le document indique que les elfes ont une augmentation de caractéristique de 2 en Dextérité et qu'ils atteignent leur maturité physique à l'âge de 100 ans, avec une espérance de vie moyenne de 750 ans.\\n\\nEn conclusion, malgré le manque de contenus fournis, je peux fournir quelques informations générales sur les variantes raciales des elfes dans Donjons et Dragons. Cependant, pour obtenir plus d'informations spécifiques, il conviendrait de consulter les manuels officiels ou les ressources en ligne dédiées à Donjons et Dragons.\\n\\nConfirmer :\\n\\nLa réponse révisée est concise, précise et contextuellement pertinente. Elle répond pleinement à la question et fournit des informations générales sur les variantes raciales des elfes dans Donjons et Dragons.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = qa_chain.invoke(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is: Bonjour ! Je vais analyser votre réponse et identifier les points forts et les points faibles.\n",
      "\n",
      "Points forts :\n",
      "\n",
      "* Vous avez identifié le contexte de la question, qui concerne les variantes raciales dans Donjons et Dragons.\n",
      "* Vous avez mentionné que vous ne pouvez pas fournir une réponse spécifique à cette question en raison du manque de contenus fournis.\n",
      "* Vous avez proposé d'obtenir plus d'informations sur les variantes raciales en consultant les manuels officiels ou les ressources en ligne dédiées à Donjons et Dragons.\n",
      "\n",
      "Points faibles :\n",
      "\n",
      "* Votre réponse est très générale et ne répond pas pleinement à la question. Vous avez mentionné que vous ne pouvez pas fournir une réponse spécifique, mais vous n'avez pas expliqué pourquoi.\n",
      "* Vous n'avez pas analysé les informations contenues dans le document fourni pour identifier les variantes raciales des elfes.\n",
      "* Votre réponse manque de concision et de précision. Vous avez mentionné plusieurs points sans fournir d'explications détaillées.\n",
      "\n",
      "Révision :\n",
      "\n",
      "Pour répondre pleinement à la question, je vais réviser ma réponse en conséquence. Voici une nouvelle réponse qui est concise, précise et contextuellement pertinente :\n",
      "\n",
      "La question demande des informations sur les variantes raciales des elfes dans Donjons et Dragons. Selon le document fourni, il n'y a pas de variante raciale spécifique mentionnée pour les elfes. Cependant, le document indique que les elfes ont une augmentation de caractéristique de 2 en Dextérité et qu'ils atteignent leur maturité physique à l'âge de 100 ans, avec une espérance de vie moyenne de 750 ans.\n",
      "\n",
      "En conclusion, malgré le manque de contenus fournis, je peux fournir quelques informations générales sur les variantes raciales des elfes dans Donjons et Dragons. Cependant, pour obtenir plus d'informations spécifiques, il conviendrait de consulter les manuels officiels ou les ressources en ligne dédiées à Donjons et Dragons.\n",
      "\n",
      "Confirmer :\n",
      "\n",
      "La réponse révisée est concise, précise et contextuellement pertinente. Elle répond pleinement à la question et fournit des informations générales sur les variantes raciales des elfes dans Donjons et Dragons. [Sources: docs_fr/races_fr_demo.pdf (page 0), docs_fr/races_fr_demo.pdf (page 0)]\n"
     ]
    }
   ],
   "source": [
    "sources = []\n",
    "for doc in retrieved_documents:\n",
    "    source = doc.metadata.get('source', 'Unknown source')\n",
    "    page = doc.metadata.get('page', 'Unknown page')\n",
    "    sources.append(f\"{source} (page {page})\")\n",
    "\n",
    "sources_text = \", \".join(sources)\n",
    "output = f\"The answer is: {results} [Sources: {sources_text}]\"\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://2c9084f8a5b0dbe4d1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2c9084f8a5b0dbe4d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_25631/1758264001.py\", line 40, in answer_question\n",
      "    results = qa_chain.invoke(query)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 385, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 750, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 944, in generate\n",
      "    output = self._generate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 787, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 774, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 1510, in _generate\n",
      "    else self._call(prompt, stop=stop, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_25631/2258379487.py\", line 20, in _call\n",
      "    response = client.complete(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py\", line 294, in wrapped_llm_predict\n",
      "    f_return_val = f(_self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/llama_index/llms/ollama/base.py\", line 185, in complete\n",
      "    response.raise_for_status()\n",
      "  File \"/home/capmars/SemAgent/.venv/lib/python3.11/site-packages/httpx/_models.py\", line 761, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "# Assuming the OpenAI client and retriever/qa_chain are already initialized\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_speech(text):\n",
    "    \"\"\"\n",
    "    Generate speech using OpenAI's TTS API.\n",
    "    \"\"\"\n",
    "    # Generate speech using OpenAI's TTS API without streaming response\n",
    "    speech = client.audio.speech.create(\n",
    "        model=\"tts-1\",  # Use the appropriate TTS model\n",
    "        voice=\"alloy\",  # Choose the preferred voice\n",
    "        input=text,\n",
    "        response_format=\"mp3\"  # Specify the audio response format\n",
    "    )\n",
    "\n",
    "    # Get the binary response content (audio data)\n",
    "    audio_data = speech.content  # Access the 'content' attribute\n",
    "\n",
    "    # Define the audio file path based on the current working directory\n",
    "    audio_file_path = os.path.join(os.getcwd(), \"response_audio.mp3\")\n",
    "    \n",
    "    # Write the binary content to the audio file\n",
    "    with open(audio_file_path, \"wb\") as f:\n",
    "        f.write(audio_data)  # Write the raw binary data\n",
    "\n",
    "    # Return the path to the saved audio file\n",
    "    return audio_file_path\n",
    "\n",
    "def answer_question(query):\n",
    "    \"\"\"\n",
    "    Answer a question and return both text and audio responses.\n",
    "    \"\"\"\n",
    "    # Retrieve documents based on the query\n",
    "    retrieved_documents = retriever.invoke(query)\n",
    "    \n",
    "    # Get the result from the QA chain\n",
    "    results = qa_chain.invoke(query)\n",
    "    \n",
    "    # Extract sources from the retrieved documents\n",
    "    sources = []\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown source')\n",
    "        page = doc.metadata.get('page', 'Unknown page')\n",
    "        sources.append(f\"{source} (page {page})\")\n",
    "\n",
    "    # Format the sources as a text string\n",
    "    sources_text = \", \".join(sources)\n",
    "    \n",
    "    # Prepare the output text\n",
    "    output_text = f\"{results} [Sources: {sources_text}]\"\n",
    "    \n",
    "    # Generate the TTS audio file\n",
    "    audio_file_path = generate_speech(results)\n",
    "    \n",
    "    # Return the text and audio file path\n",
    "    return output_text, audio_file_path\n",
    "\n",
    "# Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=\"text\",\n",
    "    outputs=[\"text\", \"audio\"],  # Output both text and audio\n",
    "    title=\"DnD ChatBot with TTS\",\n",
    "    description=\"Ask questions and receive answers with sources, along with an audio response.\"\n",
    ")\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query):\n",
    "    # Retrieve documents based on the query\n",
    "    retrieved_documents = retriever.invoke(query)\n",
    "    \n",
    "    # Get the result from the QA chain\n",
    "    results = qa_chain.invoke(query)\n",
    "    \n",
    "    # Extract sources from the retrieved documents\n",
    "    sources = []\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown source')\n",
    "        page = doc.metadata.get('page', 'Unknown page')\n",
    "        sources.append(f\"{source} (page {page})\")\n",
    "\n",
    "    # Format the sources as a text string\n",
    "    sources_text = \", \".join(sources)\n",
    "    \n",
    "    # Prepare the output\n",
    "    output = f\"{results} [Sources: {sources_text}]\"\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://68c000aa2a6b274052.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://68c000aa2a6b274052.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"DnD ChatBot\"\n",
    ")\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
