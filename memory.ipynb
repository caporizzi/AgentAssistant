{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms.base import LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom LangChain wrapper for the Ollama LLaMA model\n",
    "class OllamaLangChain(LLM):\n",
    "    model_name: str = \"llama3.2:latest\"  # Declared field\n",
    "    request_timeout: int = 30           # Declared field\n",
    "    \n",
    "\n",
    "    def __init__(self, model_name=\"llama3.2:latest\", request_timeout=30):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.request_timeout = request_timeout\n",
    "        \n",
    "        # Use object.__setattr__ to bypass Pydantic's field validation\n",
    "        object.__setattr__(self, \"_client\", Ollama(model=model_name, request_timeout=request_timeout))\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"ollama\"\n",
    "\n",
    "    def _call(self, prompt, stop=None):\n",
    "        # Access the private client\n",
    "        client = object.__getattribute__(self, \"_client\")\n",
    "        # Pass the max_tokens parameter to control the response length\n",
    "        response = client.complete(prompt)\n",
    "        return response.text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangChain components\n",
    "llm = OllamaLangChain()\n",
    "memory = ConversationBufferMemory()  # Store chat history\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat function\n",
    "def chat_with_langchain(input_text):\n",
    "    # Use LangChain's ConversationChain\n",
    "    response = conversation.run(input_text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://919e01153b4b5b0420.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://919e01153b4b5b0420.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello the number 43 has a color cyan\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hello the number 43 has a color cyan\n",
      "AI: I think there may be some confusion here! The number 43 itself doesn't have a color associated with it. However, I can tell you that the color cyan is often used to represent various things, such as wavelengths of light in the visible spectrum (around 520-560 nanometers), or it might be used in branding and design for certain companies or products.\n",
      "\n",
      "If we're talking about something specific that's related to the number 43, I'd love to hear more about it! Is there a particular context or topic you'd like to discuss?\n",
      "Human: what color did i talk about eariler\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=chat_with_langchain,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"LLaMA 3 Chatbot with LangChain\",\n",
    "    description=\"Chat with a LLaMA 3-based model via Ollama and LangChain!\"\n",
    ")\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
